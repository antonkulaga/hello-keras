{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cwd = os.getcwd()\n",
    "args = {\n",
    "    \"dataset\": \"/data/training/simple\",\n",
    "    \"plot\": os.path.join(cwd, \"plot\"),\n",
    "    \"model\": os.path.join(\"/data/training/hello-keras\",\"dense.h5\"),\n",
    "    \"label_bin\": os.path.join(\"/data/training/hello-keras\",\"labels.pkl\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# initialize the data and labels\n",
    "print(\"[INFO] loading images...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "imagePaths = sorted(list(paths.list_images(args[\"dataset\"])))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "  \n",
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "    image = cv2.imread(imagePath)\n",
    "    try:   \n",
    "        # load the image, resize the image to be 32x32 pixels (ignoring\n",
    "        # aspect ratio), flatten the image into 32x32x3=3072 pixel image\n",
    "        # into a list, and store the image in the data list\n",
    "        image = cv2.resize(image, (64, 64)).flatten()\n",
    "        data.append(image)\n",
    "    \n",
    "        # extract the class label from the image path and update the\n",
    "        # labels list\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        labels.append(label)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(imagePath)\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y has 0 samples: array([], dtype=float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ffcb0e123780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# vector)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/miniconda3/envs/ml/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mShape\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mproblems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \"\"\"\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/miniconda3/envs/ml/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    408\u001b[0m                              \"label binarization\")\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y has 0 samples: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y has 0 samples: array([], dtype=float64)"
     ]
    }
   ],
   "source": [
    "# convert the labels from integers to vectors (for 2-class, binary\n",
    "# classification you should use Keras' to_categorical function\n",
    "# instead as the scikit-learn's LabelBinarizer will not return a\n",
    "# vector)\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define the 3072-1024-512-3 architecture using Keras\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.1, input_shape=(12288,)))\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(128, activation=\"sigmoid\"))\n",
    "model.add(Dense(len(lb.classes_), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    }
   ],
   "source": [
    "# initialize our initial learning rate and # of epochs to train for\n",
    "INIT_LR = 0.01\n",
    "EPOCHS = 150\n",
    "\n",
    "# compile the model using SGD as our optimizer and categorical\n",
    "# cross-entropy loss (you'll want to use binary_crossentropy\n",
    "# for 2-class classification)\n",
    "print(\"[INFO] training network...\")\n",
    "opt = SGD(lr=INIT_LR)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3840 samples, validate on 960 samples\n",
      "Epoch 1/150\n",
      "3840/3840 [==============================] - 0s 118us/step - loss: 1.3360 - acc: 0.3120 - val_loss: 1.3407 - val_acc: 0.2812\n",
      "Epoch 2/150\n",
      "3840/3840 [==============================] - 0s 91us/step - loss: 1.3360 - acc: 0.3190 - val_loss: 1.3405 - val_acc: 0.2938\n",
      "Epoch 3/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3359 - acc: 0.3154 - val_loss: 1.3408 - val_acc: 0.2875\n",
      "Epoch 4/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3358 - acc: 0.3117 - val_loss: 1.3405 - val_acc: 0.2854\n",
      "Epoch 5/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3357 - acc: 0.3146 - val_loss: 1.3407 - val_acc: 0.2885\n",
      "Epoch 6/150\n",
      "3840/3840 [==============================] - 0s 97us/step - loss: 1.3356 - acc: 0.3161 - val_loss: 1.3408 - val_acc: 0.2875\n",
      "Epoch 7/150\n",
      "3840/3840 [==============================] - 0s 98us/step - loss: 1.3354 - acc: 0.3122 - val_loss: 1.3407 - val_acc: 0.2875\n",
      "Epoch 8/150\n",
      "3840/3840 [==============================] - 0s 95us/step - loss: 1.3353 - acc: 0.3109 - val_loss: 1.3401 - val_acc: 0.2792\n",
      "Epoch 9/150\n",
      "3840/3840 [==============================] - 0s 89us/step - loss: 1.3352 - acc: 0.3109 - val_loss: 1.3402 - val_acc: 0.2875\n",
      "Epoch 10/150\n",
      "3840/3840 [==============================] - 0s 89us/step - loss: 1.3353 - acc: 0.3094 - val_loss: 1.3400 - val_acc: 0.2885\n",
      "Epoch 11/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3350 - acc: 0.3109 - val_loss: 1.3394 - val_acc: 0.2875\n",
      "Epoch 12/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.3348 - acc: 0.3089 - val_loss: 1.3393 - val_acc: 0.2906\n",
      "Epoch 13/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3350 - acc: 0.3125 - val_loss: 1.3395 - val_acc: 0.2854\n",
      "Epoch 14/150\n",
      "3840/3840 [==============================] - 0s 98us/step - loss: 1.3346 - acc: 0.3154 - val_loss: 1.3398 - val_acc: 0.2875\n",
      "Epoch 15/150\n",
      "3840/3840 [==============================] - 0s 92us/step - loss: 1.3345 - acc: 0.3112 - val_loss: 1.3397 - val_acc: 0.2875\n",
      "Epoch 16/150\n",
      "3840/3840 [==============================] - 0s 92us/step - loss: 1.3343 - acc: 0.3107 - val_loss: 1.3389 - val_acc: 0.2854\n",
      "Epoch 17/150\n",
      "3840/3840 [==============================] - 0s 89us/step - loss: 1.3344 - acc: 0.3122 - val_loss: 1.3385 - val_acc: 0.2833\n",
      "Epoch 18/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.3341 - acc: 0.3138 - val_loss: 1.3386 - val_acc: 0.2854\n",
      "Epoch 19/150\n",
      "3840/3840 [==============================] - 0s 95us/step - loss: 1.3338 - acc: 0.3109 - val_loss: 1.3389 - val_acc: 0.2875\n",
      "Epoch 20/150\n",
      "3840/3840 [==============================] - 0s 93us/step - loss: 1.3339 - acc: 0.3086 - val_loss: 1.3386 - val_acc: 0.2875\n",
      "Epoch 21/150\n",
      "3840/3840 [==============================] - 0s 101us/step - loss: 1.3337 - acc: 0.3107 - val_loss: 1.3383 - val_acc: 0.2875\n",
      "Epoch 22/150\n",
      "3840/3840 [==============================] - 0s 95us/step - loss: 1.3336 - acc: 0.3122 - val_loss: 1.3385 - val_acc: 0.2854\n",
      "Epoch 23/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.3335 - acc: 0.3107 - val_loss: 1.3382 - val_acc: 0.2885\n",
      "Epoch 24/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3332 - acc: 0.3120 - val_loss: 1.3376 - val_acc: 0.2927\n",
      "Epoch 25/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.3331 - acc: 0.3227 - val_loss: 1.3379 - val_acc: 0.2875\n",
      "Epoch 26/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3331 - acc: 0.3102 - val_loss: 1.3379 - val_acc: 0.2875\n",
      "Epoch 27/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3328 - acc: 0.3107 - val_loss: 1.3379 - val_acc: 0.2875\n",
      "Epoch 28/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3327 - acc: 0.3120 - val_loss: 1.3375 - val_acc: 0.2885\n",
      "Epoch 29/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3324 - acc: 0.3112 - val_loss: 1.3367 - val_acc: 0.2802\n",
      "Epoch 30/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3323 - acc: 0.3120 - val_loss: 1.3364 - val_acc: 0.2927\n",
      "Epoch 31/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3323 - acc: 0.3141 - val_loss: 1.3368 - val_acc: 0.2875\n",
      "Epoch 32/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3321 - acc: 0.3211 - val_loss: 1.3373 - val_acc: 0.2875\n",
      "Epoch 33/150\n",
      "3840/3840 [==============================] - 0s 83us/step - loss: 1.3322 - acc: 0.3109 - val_loss: 1.3366 - val_acc: 0.2875\n",
      "Epoch 34/150\n",
      "3840/3840 [==============================] - 0s 83us/step - loss: 1.3317 - acc: 0.3125 - val_loss: 1.3363 - val_acc: 0.2969\n",
      "Epoch 35/150\n",
      "3840/3840 [==============================] - 0s 92us/step - loss: 1.3318 - acc: 0.3146 - val_loss: 1.3357 - val_acc: 0.2802\n",
      "Epoch 36/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.3314 - acc: 0.3169 - val_loss: 1.3361 - val_acc: 0.2854\n",
      "Epoch 37/150\n",
      "3840/3840 [==============================] - 0s 95us/step - loss: 1.3313 - acc: 0.3151 - val_loss: 1.3359 - val_acc: 0.2875\n",
      "Epoch 38/150\n",
      "3840/3840 [==============================] - 0s 94us/step - loss: 1.3312 - acc: 0.3109 - val_loss: 1.3357 - val_acc: 0.2854\n",
      "Epoch 39/150\n",
      "3840/3840 [==============================] - 0s 92us/step - loss: 1.3310 - acc: 0.3172 - val_loss: 1.3355 - val_acc: 0.2885\n",
      "Epoch 40/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3309 - acc: 0.3107 - val_loss: 1.3352 - val_acc: 0.2781\n",
      "Epoch 41/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3306 - acc: 0.3076 - val_loss: 1.3346 - val_acc: 0.2833\n",
      "Epoch 42/150\n",
      "3840/3840 [==============================] - 0s 89us/step - loss: 1.3306 - acc: 0.3164 - val_loss: 1.3344 - val_acc: 0.2865\n",
      "Epoch 43/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3303 - acc: 0.3122 - val_loss: 1.3344 - val_acc: 0.2802\n",
      "Epoch 44/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3303 - acc: 0.3115 - val_loss: 1.3340 - val_acc: 0.2802\n",
      "Epoch 45/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3299 - acc: 0.3130 - val_loss: 1.3342 - val_acc: 0.2854\n",
      "Epoch 46/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3299 - acc: 0.3107 - val_loss: 1.3336 - val_acc: 0.2906\n",
      "Epoch 47/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3296 - acc: 0.3138 - val_loss: 1.3337 - val_acc: 0.2865\n",
      "Epoch 48/150\n",
      "3840/3840 [==============================] - 0s 91us/step - loss: 1.3294 - acc: 0.3112 - val_loss: 1.3333 - val_acc: 0.2792\n",
      "Epoch 49/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.3292 - acc: 0.3146 - val_loss: 1.3327 - val_acc: 0.3177\n",
      "Epoch 50/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3291 - acc: 0.3263 - val_loss: 1.3334 - val_acc: 0.2875\n",
      "Epoch 51/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3289 - acc: 0.3120 - val_loss: 1.3331 - val_acc: 0.2854\n",
      "Epoch 52/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3286 - acc: 0.3102 - val_loss: 1.3330 - val_acc: 0.2854\n",
      "Epoch 53/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3285 - acc: 0.3107 - val_loss: 1.3322 - val_acc: 0.2813\n",
      "Epoch 54/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3283 - acc: 0.3172 - val_loss: 1.3328 - val_acc: 0.2875\n",
      "Epoch 55/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3282 - acc: 0.3089 - val_loss: 1.3323 - val_acc: 0.2875\n",
      "Epoch 56/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3279 - acc: 0.3104 - val_loss: 1.3321 - val_acc: 0.2875\n",
      "Epoch 57/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3276 - acc: 0.3120 - val_loss: 1.3319 - val_acc: 0.2885\n",
      "Epoch 58/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3275 - acc: 0.3161 - val_loss: 1.3316 - val_acc: 0.2854\n",
      "Epoch 59/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3273 - acc: 0.3141 - val_loss: 1.3316 - val_acc: 0.2875\n",
      "Epoch 60/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3272 - acc: 0.3135 - val_loss: 1.3312 - val_acc: 0.2875\n",
      "Epoch 61/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3268 - acc: 0.3096 - val_loss: 1.3308 - val_acc: 0.2875\n",
      "Epoch 62/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3266 - acc: 0.3128 - val_loss: 1.3308 - val_acc: 0.2854\n",
      "Epoch 63/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3266 - acc: 0.3125 - val_loss: 1.3296 - val_acc: 0.2979\n",
      "Epoch 64/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3263 - acc: 0.3133 - val_loss: 1.3299 - val_acc: 0.2802\n",
      "Epoch 65/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3261 - acc: 0.3180 - val_loss: 1.3299 - val_acc: 0.2854\n",
      "Epoch 66/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3259 - acc: 0.3112 - val_loss: 1.3298 - val_acc: 0.2865\n",
      "Epoch 67/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3256 - acc: 0.3125 - val_loss: 1.3298 - val_acc: 0.2875\n",
      "Epoch 68/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3252 - acc: 0.3107 - val_loss: 1.3292 - val_acc: 0.2865\n",
      "Epoch 69/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3255 - acc: 0.3086 - val_loss: 1.3286 - val_acc: 0.2781\n",
      "Epoch 70/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3249 - acc: 0.3130 - val_loss: 1.3288 - val_acc: 0.2875\n",
      "Epoch 71/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3247 - acc: 0.3125 - val_loss: 1.3288 - val_acc: 0.2885\n",
      "Epoch 72/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3244 - acc: 0.3125 - val_loss: 1.3278 - val_acc: 0.2875\n",
      "Epoch 73/150\n",
      "3840/3840 [==============================] - 0s 92us/step - loss: 1.3241 - acc: 0.3135 - val_loss: 1.3276 - val_acc: 0.2844\n",
      "Epoch 74/150\n",
      "3840/3840 [==============================] - 0s 89us/step - loss: 1.3239 - acc: 0.3128 - val_loss: 1.3273 - val_acc: 0.2771\n",
      "Epoch 75/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3237 - acc: 0.3125 - val_loss: 1.3266 - val_acc: 0.2906\n",
      "Epoch 76/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3233 - acc: 0.3096 - val_loss: 1.3268 - val_acc: 0.2802\n",
      "Epoch 77/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.3231 - acc: 0.3146 - val_loss: 1.3265 - val_acc: 0.2875\n",
      "Epoch 78/150\n",
      "3840/3840 [==============================] - 0s 91us/step - loss: 1.3230 - acc: 0.3130 - val_loss: 1.3262 - val_acc: 0.2781\n",
      "Epoch 79/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.3225 - acc: 0.3135 - val_loss: 1.3262 - val_acc: 0.2885\n",
      "Epoch 80/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3222 - acc: 0.3336 - val_loss: 1.3263 - val_acc: 0.2854\n",
      "Epoch 81/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3223 - acc: 0.3031 - val_loss: 1.3257 - val_acc: 0.3000\n",
      "Epoch 82/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3218 - acc: 0.3214 - val_loss: 1.3251 - val_acc: 0.2833\n",
      "Epoch 83/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3215 - acc: 0.3128 - val_loss: 1.3247 - val_acc: 0.2802\n",
      "Epoch 84/150\n",
      "3840/3840 [==============================] - 0s 92us/step - loss: 1.3212 - acc: 0.3167 - val_loss: 1.3246 - val_acc: 0.3000\n",
      "Epoch 85/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3212 - acc: 0.3224 - val_loss: 1.3239 - val_acc: 0.2875\n",
      "Epoch 86/150\n",
      "3840/3840 [==============================] - 0s 89us/step - loss: 1.3208 - acc: 0.3201 - val_loss: 1.3242 - val_acc: 0.2906\n",
      "Epoch 87/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3206 - acc: 0.3281 - val_loss: 1.3239 - val_acc: 0.2865\n",
      "Epoch 88/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3203 - acc: 0.3182 - val_loss: 1.3239 - val_acc: 0.2990\n",
      "Epoch 89/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3200 - acc: 0.3227 - val_loss: 1.3226 - val_acc: 0.2927\n",
      "Epoch 90/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3196 - acc: 0.3253 - val_loss: 1.3223 - val_acc: 0.2958\n",
      "Epoch 91/150\n",
      "3840/3840 [==============================] - 0s 99us/step - loss: 1.3193 - acc: 0.3234 - val_loss: 1.3219 - val_acc: 0.3000\n",
      "Epoch 92/150\n",
      "3840/3840 [==============================] - 0s 97us/step - loss: 1.3190 - acc: 0.3336 - val_loss: 1.3216 - val_acc: 0.2917\n",
      "Epoch 93/150\n",
      "3840/3840 [==============================] - 0s 95us/step - loss: 1.3187 - acc: 0.3247 - val_loss: 1.3212 - val_acc: 0.3167\n",
      "Epoch 94/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3184 - acc: 0.3276 - val_loss: 1.3211 - val_acc: 0.3187\n",
      "Epoch 95/150\n",
      "3840/3840 [==============================] - 0s 83us/step - loss: 1.3181 - acc: 0.3344 - val_loss: 1.3211 - val_acc: 0.3104\n",
      "Epoch 96/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3178 - acc: 0.3393 - val_loss: 1.3207 - val_acc: 0.2844\n",
      "Epoch 97/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3173 - acc: 0.3310 - val_loss: 1.3202 - val_acc: 0.3104\n",
      "Epoch 98/150\n",
      "3840/3840 [==============================] - 0s 83us/step - loss: 1.3170 - acc: 0.3435 - val_loss: 1.3197 - val_acc: 0.2958\n",
      "Epoch 99/150\n",
      "3840/3840 [==============================] - 0s 92us/step - loss: 1.3167 - acc: 0.3302 - val_loss: 1.3196 - val_acc: 0.3312\n",
      "Epoch 100/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3165 - acc: 0.3451 - val_loss: 1.3191 - val_acc: 0.3187\n",
      "Epoch 101/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3162 - acc: 0.3497 - val_loss: 1.3190 - val_acc: 0.3073\n",
      "Epoch 102/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3159 - acc: 0.3365 - val_loss: 1.3185 - val_acc: 0.3188\n",
      "Epoch 103/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3157 - acc: 0.3482 - val_loss: 1.3182 - val_acc: 0.3167\n",
      "Epoch 104/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3152 - acc: 0.3469 - val_loss: 1.3177 - val_acc: 0.3146\n",
      "Epoch 105/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3148 - acc: 0.3578 - val_loss: 1.3174 - val_acc: 0.2969\n",
      "Epoch 106/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3144 - acc: 0.3302 - val_loss: 1.3170 - val_acc: 0.3448\n",
      "Epoch 107/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3140 - acc: 0.3521 - val_loss: 1.3161 - val_acc: 0.3417\n",
      "Epoch 108/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3139 - acc: 0.3539 - val_loss: 1.3160 - val_acc: 0.3365\n",
      "Epoch 109/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3134 - acc: 0.3523 - val_loss: 1.3157 - val_acc: 0.3375\n",
      "Epoch 110/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3131 - acc: 0.3503 - val_loss: 1.3152 - val_acc: 0.3469\n",
      "Epoch 111/150\n",
      "3840/3840 [==============================] - 0s 83us/step - loss: 1.3126 - acc: 0.3544 - val_loss: 1.3147 - val_acc: 0.3510\n",
      "Epoch 112/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3123 - acc: 0.3719 - val_loss: 1.3146 - val_acc: 0.3427\n",
      "Epoch 113/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3122 - acc: 0.3544 - val_loss: 1.3138 - val_acc: 0.3448\n",
      "Epoch 114/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3114 - acc: 0.3625 - val_loss: 1.3136 - val_acc: 0.3333\n",
      "Epoch 115/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3110 - acc: 0.3539 - val_loss: 1.3131 - val_acc: 0.3510\n",
      "Epoch 116/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3106 - acc: 0.3620 - val_loss: 1.3123 - val_acc: 0.3510\n",
      "Epoch 117/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3105 - acc: 0.3773 - val_loss: 1.3126 - val_acc: 0.3531\n",
      "Epoch 118/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3097 - acc: 0.3714 - val_loss: 1.3117 - val_acc: 0.3438\n",
      "Epoch 119/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3095 - acc: 0.3674 - val_loss: 1.3114 - val_acc: 0.3427\n",
      "Epoch 120/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3091 - acc: 0.3573 - val_loss: 1.3108 - val_acc: 0.3500\n",
      "Epoch 121/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3088 - acc: 0.3708 - val_loss: 1.3100 - val_acc: 0.3604\n",
      "Epoch 122/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3083 - acc: 0.3732 - val_loss: 1.3102 - val_acc: 0.3427\n",
      "Epoch 123/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3078 - acc: 0.3693 - val_loss: 1.3093 - val_acc: 0.3521\n",
      "Epoch 124/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3074 - acc: 0.3701 - val_loss: 1.3088 - val_acc: 0.3563\n",
      "Epoch 125/150\n",
      "3840/3840 [==============================] - 0s 85us/step - loss: 1.3070 - acc: 0.3721 - val_loss: 1.3082 - val_acc: 0.3563\n",
      "Epoch 126/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3065 - acc: 0.3672 - val_loss: 1.3080 - val_acc: 0.3573\n",
      "Epoch 127/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3060 - acc: 0.3763 - val_loss: 1.3072 - val_acc: 0.3573\n",
      "Epoch 128/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3060 - acc: 0.3695 - val_loss: 1.3065 - val_acc: 0.3667\n",
      "Epoch 129/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3052 - acc: 0.3781 - val_loss: 1.3064 - val_acc: 0.3531\n",
      "Epoch 130/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3047 - acc: 0.3779 - val_loss: 1.3059 - val_acc: 0.3542\n",
      "Epoch 131/150\n",
      "3840/3840 [==============================] - 0s 89us/step - loss: 1.3042 - acc: 0.3755 - val_loss: 1.3054 - val_acc: 0.3604\n",
      "Epoch 132/150\n",
      "3840/3840 [==============================] - 0s 89us/step - loss: 1.3036 - acc: 0.3734 - val_loss: 1.3045 - val_acc: 0.3656\n",
      "Epoch 133/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3033 - acc: 0.3755 - val_loss: 1.3045 - val_acc: 0.3656\n",
      "Epoch 134/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3027 - acc: 0.3799 - val_loss: 1.3043 - val_acc: 0.3656\n",
      "Epoch 135/150\n",
      "3840/3840 [==============================] - 0s 84us/step - loss: 1.3021 - acc: 0.3779 - val_loss: 1.3028 - val_acc: 0.3688\n",
      "Epoch 136/150\n",
      "3840/3840 [==============================] - 0s 86us/step - loss: 1.3017 - acc: 0.3802 - val_loss: 1.3029 - val_acc: 0.3677\n",
      "Epoch 137/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3014 - acc: 0.3794 - val_loss: 1.3019 - val_acc: 0.3615\n",
      "Epoch 138/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.3008 - acc: 0.3805 - val_loss: 1.3014 - val_acc: 0.3625\n",
      "Epoch 139/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.3004 - acc: 0.3763 - val_loss: 1.3008 - val_acc: 0.3635\n",
      "Epoch 140/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.2996 - acc: 0.3750 - val_loss: 1.3000 - val_acc: 0.3677\n",
      "Epoch 141/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.2992 - acc: 0.3812 - val_loss: 1.2999 - val_acc: 0.3646\n",
      "Epoch 142/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.2986 - acc: 0.3768 - val_loss: 1.2991 - val_acc: 0.3625\n",
      "Epoch 143/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.2982 - acc: 0.3812 - val_loss: 1.2985 - val_acc: 0.3625\n",
      "Epoch 144/150\n",
      "3840/3840 [==============================] - 0s 89us/step - loss: 1.2977 - acc: 0.3789 - val_loss: 1.2977 - val_acc: 0.3635\n",
      "Epoch 145/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.2970 - acc: 0.3802 - val_loss: 1.2971 - val_acc: 0.3667\n",
      "Epoch 146/150\n",
      "3840/3840 [==============================] - 0s 93us/step - loss: 1.2965 - acc: 0.3839 - val_loss: 1.2966 - val_acc: 0.3635\n",
      "Epoch 147/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.2959 - acc: 0.3818 - val_loss: 1.2961 - val_acc: 0.3656\n",
      "Epoch 148/150\n",
      "3840/3840 [==============================] - 0s 90us/step - loss: 1.2952 - acc: 0.3818 - val_loss: 1.2956 - val_acc: 0.3604\n",
      "Epoch 149/150\n",
      "3840/3840 [==============================] - 0s 88us/step - loss: 1.2947 - acc: 0.3820 - val_loss: 1.2945 - val_acc: 0.3802\n",
      "Epoch 150/150\n",
      "3840/3840 [==============================] - 0s 87us/step - loss: 1.2941 - acc: 0.3812 - val_loss: 1.2938 - val_acc: 0.3750\n"
     ]
    }
   ],
   "source": [
    "# train the neural network\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=EPOCHS, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=256)\n",
    "#report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_)\n",
    "#print(report)\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (Simple NN)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plot\") #args[\"plot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing network and label binarizer...\n"
     ]
    }
   ],
   "source": [
    "# save the model and label binarizer to disk\n",
    "print(\"[INFO] serializing network and label binarizer...\")\n",
    "model.save(args[\"model\"])\n",
    "f = open(args[\"label_bin\"], \"wb\")\n",
    "f.write(pickle.dumps(lb))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
